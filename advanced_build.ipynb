{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d1e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from typing import TypedDict, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5390212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define State\n",
    "class PostState(TypedDict):\n",
    "    messages: list\n",
    "    summary: str\n",
    "    post: str\n",
    "    verify_result: Literal[\"pass\", \"revise\"]\n",
    "    revision_count: int\n",
    "    tech_check: str\n",
    "    style_check: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce7dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Tools\n",
    "@tool\n",
    "def mock_paper_lookup(title: str) -> str:\n",
    "    \"\"\"Simulate paper retrieval and return a short abstract.\"\"\"\n",
    "    return (\n",
    "        \"LoRA is a low-rank adaptation technique for fine-tuning large language models. \"\n",
    "        \"It enables efficient training by injecting low-rank matrices into transformer weights.\"\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def verify_technical_correctness(summary: str) -> str:\n",
    "    \"\"\"Returns 'pass' or 'revise' for technical accuracy (simulated).\"\"\"\n",
    "    return \"pass\" if \"LoRA\" in summary else \"revise\"\n",
    "\n",
    "@tool\n",
    "def check_social_post_style(post: str) -> str:\n",
    "    \"\"\"Check post for platform-appropriate style. Simulated.\"\"\"\n",
    "    if len(post) > 300 or \"@AIMakerspace\" not in post:\n",
    "        return \"revise\"\n",
    "    return \"pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1b4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Agent Executors\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def create_tool_agent(system_prompt: str, tools: list):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"messages\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ])\n",
    "    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    return AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "def create_prompt_only_agent(system_prompt: str, output_key: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "\n",
    "    def run(state: dict):\n",
    "        # Ensure messages is a proper list of LangChain message objects\n",
    "        messages = state.get(\"messages\", [])\n",
    "        if not isinstance(messages, list):\n",
    "            raise ValueError(\"Expected 'messages' to be a list\")\n",
    "        response = chain.invoke({\"messages\": messages})\n",
    "        return {\n",
    "            **state,\n",
    "            output_key: response.content\n",
    "        }\n",
    "\n",
    "    return RunnableLambda(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a24f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Agents\n",
    "research_agent = create_tool_agent(\n",
    "    \"You are a researcher. Use available tools to find ML paper content.\", [mock_paper_lookup]\n",
    ")\n",
    "\n",
    "summarize_agent = create_prompt_only_agent(\n",
    "    \"You are a scientific summarizer. Generate a short, accurate summary.\", \"summary\"\n",
    ")\n",
    "\n",
    "post_agent = create_prompt_only_agent(\n",
    "    \"You are a social media strategist. Write a LinkedIn post using the summary. The post MUST mention @AIMakerspace and MUST be under 300 characters. If you are revising, make sure to fix any issues with these requirements.\",\n",
    "    \"post\"\n",
    ")\n",
    "\n",
    "verify_agent = create_tool_agent(\n",
    "    \"You are a verification team. Check both accuracy and platform style.\", \n",
    "    [verify_technical_correctness, check_social_post_style]\n",
    ")\n",
    "\n",
    "def verify_wrapper(state):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    post = state.get(\"post\", \"\")\n",
    "    tech_result = verify_technical_correctness(summary)\n",
    "    style_result = check_social_post_style(post)\n",
    "    revision_count = state.get(\"revision_count\", 0)\n",
    "    if tech_result == \"pass\" and style_result == \"pass\":\n",
    "        verify_result = \"pass\"\n",
    "    elif revision_count >= 3:\n",
    "        verify_result = \"pass\"\n",
    "    else:\n",
    "        verify_result = \"revise\"\n",
    "    return {\n",
    "        **state,\n",
    "        \"verify_result\": verify_result,\n",
    "        \"revision_count\": revision_count + 1,\n",
    "        \"tech_check\": tech_result,\n",
    "        \"style_check\": style_result\n",
    "    }\n",
    "\n",
    "verify_node = RunnableLambda(verify_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15c48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Router\n",
    "def verify_output_router(state: PostState) -> Literal[\"pass\", \"revise\"]:\n",
    "    return state[\"verify_result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9955ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Graph\n",
    "graph = StateGraph(PostState)\n",
    "\n",
    "graph.add_node(\"research\", research_agent)\n",
    "graph.add_node(\"summarize\", summarize_agent)\n",
    "graph.add_node(\"draft_post\", post_agent)\n",
    "graph.add_node(\"verify\", verify_node)\n",
    "\n",
    "graph.set_entry_point(\"research\")\n",
    "graph.add_edge(\"research\", \"summarize\")\n",
    "graph.add_edge(\"summarize\", \"draft_post\")\n",
    "graph.add_edge(\"draft_post\", \"verify\")\n",
    "\n",
    "graph.add_conditional_edges(\"verify\", verify_output_router, {\n",
    "    \"pass\": END,\n",
    "    \"revise\": \"draft_post\"\n",
    "})\n",
    "\n",
    "advanced_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b9fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Output:\n",
      "Please write a LinkedIn post about the paper 'LoRA: Low-Rank Adaptation of LLMs'\n",
      "\n",
      "📄 Summary:\n",
      " 🚀 Exciting Advances in AI: Low-Rank Adaptation of LLMs 🌟\n",
      "\n",
      "I'm thrilled to share insights from the recently published paper on LoRA: Low-Rank Adaptation of Large Language Models! 🤖\n",
      "\n",
      "As we push the boundaries of AI, the computational demands of fine-tuning large language models (LLMs) have become a challenge. LoRA offers an innovative solution by reducing these demands through low-rank adaptation, enabling efficient and scalable model customization without compromising performance.\n",
      "\n",
      "LoRA introduces a novel technique that adapts LLMs by adjusting model parameters within a low-rank space, significantly reducing the number of trainable parameters. This approach not only lowers computational costs but also facilitates model deployment in resource-constrained environments.\n",
      "\n",
      "Key takeaways:\n",
      "- Enhanced efficiency in adapting large models\n",
      "- Reduced computational resource requirements\n",
      "- Improved scalability for broad applications\n",
      "\n",
      "This breakthrough holds great potential for democratizing access to AI technologies and advancing applications across various industries. A big congratulations to the authors for this significant contribution to AI research! 🎉\n",
      "\n",
      "#AI #MachineLearning #LLM #LoRA #Innovation #FutureOfAI\n",
      "\n",
      "📝 Post:\n",
      " Excited to dive into 'LoRA: Low-Rank Adaptation of LLMs'! This groundbreaking approach optimizes large language models efficiently. Kudos to the research team for their innovative work. An essential read for AI enthusiasts! 🌟 @AIMakerspace #AI #MachineLearning #Innovation \n",
      "\n",
      "🔍 Verify Result: pass\n",
      "\n",
      "🔁 Number of Revisions: 1\n",
      "\n",
      "🔬 Technical Check: pass\n",
      "🎨 Style Check: pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_580009/2828212042.py:23: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  tech_result = verify_technical_correctness(summary)\n"
     ]
    }
   ],
   "source": [
    "# 7. Run Graph\n",
    "paper_title = \"LoRA: Low-Rank Adaptation of LLMs\"\n",
    "input_messages = [\n",
    "    HumanMessage(content=f\"Please write a LinkedIn post about the paper '{paper_title}'\")\n",
    "]\n",
    "\n",
    "result = advanced_graph.invoke({\"messages\": input_messages, \"revision_count\": 0})\n",
    "\n",
    "print(\"✅ Final Output:\")\n",
    "for m in result[\"messages\"]:\n",
    "    print(m.content)\n",
    "\n",
    "print(\"\\n📄 Summary:\\n\", result[\"summary\"])\n",
    "print(\"\\n📝 Post:\\n\", result[\"post\"])\n",
    "print(\"\\n🔍 Verify Result:\", result[\"verify_result\"])\n",
    "print(\"\\n🔁 Number of Revisions:\", result[\"revision_count\"])\n",
    "print(\"\\n🔬 Technical Check:\", result[\"tech_check\"])\n",
    "print(\"🎨 Style Check:\", result[\"style_check\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
